{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Originally from Make Your Own Neural Network (Rashid 2016, 2024), Adapted by Hilary Abraham, 2024, for Science World BC\n# instructions and further code found on GitHub hascienceworld/nnlesson\n# code for a 3-layer neural network, and code for learning the MNIST dataset\n# MNIST dataset converted to .csv by Joseph Redmon\n# (c) Tariq Rashid, 2016\n# license is GPLv2","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:52:05.787004Z","iopub.execute_input":"2024-10-28T17:52:05.787446Z","iopub.status.idle":"2024-10-28T17:52:05.793067Z","shell.execute_reply.started":"2024-10-28T17:52:05.787409Z","shell.execute_reply":"2024-10-28T17:52:05.791881Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# import necessary libraries\n\n# numpy library for scientific computing\nimport numpy\n# scipy.special for the sigmoid function expit()\nimport scipy.special\n# library for plotting arrays\nimport matplotlib.pyplot\n# library to interact with kaggle datasets\nimport kagglehub","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-28T17:52:05.795206Z","iopub.execute_input":"2024-10-28T17:52:05.795566Z","iopub.status.idle":"2024-10-28T17:52:05.804566Z","shell.execute_reply.started":"2024-10-28T17:52:05.795529Z","shell.execute_reply":"2024-10-28T17:52:05.802900Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# neural network class definition\n\nclass neuralNetwork:\n    \n    # initialise the neural network\n    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n        # set number of nodes in each input, hidden, output layer\n        self.inodes = inputnodes\n        self.hnodes = hiddennodes\n        self.onodes = outputnodes\n        \n        # link weight matrices, wih and who\n        # weights inside the arrays are w_i_j, where link is from node i to node j in the next layer\n        # w11 w21\n        # w12 w22 etc \n        self.wih = numpy.random.normal(0.0, pow(self.inodes, -0.5), (self.hnodes, self.inodes))\n        self.who = numpy.random.normal(0.0, pow(self.hnodes, -0.5), (self.onodes, self.hnodes))\n\n        # learning rate\n        self.lr = learningrate\n        \n        # activation function is the sigmoid function\n        self.activation_function = lambda x: scipy.special.expit(x)\n        # logit function is for backwards query\n        self.inverse_activation_function = lambda x: scipy.special.logit(x)\n        \n        pass\n\n    \n    # train the neural network\n    def train(self, inputs_list, targets_list):\n        # convert inputs list to 2d array\n        inputs = numpy.array(inputs_list, ndmin=2).T\n        targets = numpy.array(targets_list, ndmin=2).T\n        \n        # calculate signals into hidden layer\n        hidden_inputs = numpy.dot(self.wih, inputs)\n        # calculate the signals emerging from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs)\n        \n        # calculate signals into final output layer\n        final_inputs = numpy.dot(self.who, hidden_outputs)\n        # calculate the signals emerging from final output layer\n        final_outputs = self.activation_function(final_inputs)\n        \n        # output layer error is the (target - actual)\n        output_errors = targets - final_outputs\n        # hidden layer error is the output_errors, split by weights, recombined at hidden nodes\n        hidden_errors = numpy.dot(self.who.T, output_errors) \n        \n        # update the weights for the links between the hidden and output layers\n        self.who += self.lr * numpy.dot((output_errors * final_outputs * (1.0 - final_outputs)), numpy.transpose(hidden_outputs))\n        \n        # update the weights for the links between the input and hidden layers\n        self.wih += self.lr * numpy.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), numpy.transpose(inputs))\n        \n        pass\n\n    \n    # query the neural network\n    def query(self, inputs_list):\n        # convert inputs list to 2d array\n        inputs = numpy.array(inputs_list, ndmin=2).T\n        \n        # calculate signals into hidden layer\n        hidden_inputs = numpy.dot(self.wih, inputs)\n        # calculate the signals emerging from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs)\n        \n        # calculate signals into final output layer\n        final_inputs = numpy.dot(self.who, hidden_outputs)\n        # calculate the signals emerging from final output layer\n        final_outputs = self.activation_function(final_inputs)\n        \n        return final_outputs\n    \n    \n    # backquery the neural network (in Part 3)\n    def backquery(self, targets_list):\n        # transpose the targets list to a vertical array\n        final_outputs = numpy.array(targets_list, ndmin=2).T\n        \n        # calculate the signal into the final output layer\n        final_inputs = self.inverse_activation_function(final_outputs)\n\n        # calculate the signal out of the hidden layer\n        hidden_outputs = numpy.dot(self.who.T, final_inputs)\n        # scale them back to 0.01 to .99\n        hidden_outputs -= numpy.min(hidden_outputs)\n        hidden_outputs /= numpy.max(hidden_outputs)\n        hidden_outputs *= 0.98\n        hidden_outputs += 0.01\n        \n        # calculate the signal into the hidden layer\n        hidden_inputs = self.inverse_activation_function(hidden_outputs)\n        \n        # calculate the signal out of the input layer\n        inputs = numpy.dot(self.wih.T, hidden_inputs)\n        # scale them back to 0.01 to .99\n        inputs -= numpy.min(inputs)\n        inputs /= numpy.max(inputs)\n        inputs *= 0.98\n        inputs += 0.01\n        \n        return inputs\n    \n    \n    # save neural network weights (in Part 4)\n    def save(self):\n        numpy.save('saved_wih.npy', self.wih)\n        numpy.save('saved_who.npy', self.who)\n        pass","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:52:05.807074Z","iopub.execute_input":"2024-10-28T17:52:05.807572Z","iopub.status.idle":"2024-10-28T17:52:05.825049Z","shell.execute_reply.started":"2024-10-28T17:52:05.807516Z","shell.execute_reply":"2024-10-28T17:52:05.823371Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# EDIT THE NEURAL NETWORK PARAMETERS HERE\n# you can change the number of hidden nodes, the learning rate, and the number of epochs\n\n# number of input, hidden and output nodes\ninput_nodes = 784\nhidden_nodes = 10\noutput_nodes = 10\n\n# learning rate\nlearning_rate = 0.9\n\n# epochs is the number of times the training data set is used for training\nepochs = 1","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:52:05.826876Z","iopub.execute_input":"2024-10-28T17:52:05.827297Z","iopub.status.idle":"2024-10-28T17:52:05.838121Z","shell.execute_reply.started":"2024-10-28T17:52:05.827261Z","shell.execute_reply":"2024-10-28T17:52:05.836840Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# create instance of neural network\nn = neuralNetwork(input_nodes,hidden_nodes,output_nodes, learning_rate)","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:52:05.841367Z","iopub.execute_input":"2024-10-28T17:52:05.841854Z","iopub.status.idle":"2024-10-28T17:52:05.849555Z","shell.execute_reply.started":"2024-10-28T17:52:05.841815Z","shell.execute_reply":"2024-10-28T17:52:05.848053Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# load datasets\n# small dataset is for low internet speeds and fast training; for larger dataset, change \"small\" to say \"large\"\nkagglehub.dataset_download('hilaryabraham/sw-mnist-small')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the neural network:\n\n# load the mnist training data CSV file into a list\n# change the file address to use a different file\ntraining_data_file = open('/kaggle/input/sw-mnist-small/train/mnist_train_1000.csv', \"r\")\ntraining_data_list = training_data_file.readlines()\ntraining_data_file.close()\n\nfor e in range(epochs):\n    # go through all records in the training data set\n    for record in training_data_list:\n        # split the record by the ',' commas\n        all_values = record.split(',')\n        # scale and shift the inputs\n        inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n        # create the target output values (all 0.01, except the desired label which is 0.99)\n        targets = numpy.zeros(output_nodes) + 0.01\n        # all_values[0] is the target label for this record\n        targets[int(all_values[0])] = 0.99\n        n.train(inputs, targets)\n        pass\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-10-28T17:52:06.441531Z","iopub.execute_input":"2024-10-28T17:52:06.442053Z","iopub.status.idle":"2024-10-28T17:52:06.618781Z","shell.execute_reply.started":"2024-10-28T17:52:06.441998Z","shell.execute_reply":"2024-10-28T17:52:06.617506Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# test the neural network\n\n# load the mnist test data CSV file into a list\n# change the file address to use a different file\ntest_data_file = open('/kaggle/input/sw-mnist-small/test/mnist_test_100.csv', 'r')\ntest_data_list = test_data_file.readlines()\ntest_data_file.close()\n\n# scorecard for how well the network performs, initially empty\nscorecard = []\n\n# go through all the records in the test data set\nfor record in test_data_list:\n    # split the record by the ',' commas\n    all_values = record.split(',')\n    # correct answer is first value\n    correct_label = int(all_values[0])\n    # scale and shift the inputs\n    inputs = (numpy.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n    # query the network\n    outputs = n.query(inputs)\n    # the index of the highest value corresponds to the label\n    label = numpy.argmax(outputs)\n    # append correct or incorrect to list\n    if (label == correct_label):\n        # network's answer matches correct answer, add 1 to scorecard\n        scorecard.append(1)\n    else:\n        # network's answer doesn't match correct answer, add 0 to scorecard\n        scorecard.append(0)\n        pass\n    \n    pass\n\n# calculate the performance score, the fraction of correct answers\nscorecard_array = numpy.asarray(scorecard)\nprint (\"performance = \", scorecard_array.sum() / scorecard_array.size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}